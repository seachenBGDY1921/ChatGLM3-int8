import streamlit as st

st.set_page_config(
    page_title="ChatGLM3 Demo",
    page_icon=":robot:",
    layout='centered',
    initial_sidebar_state='expanded',
)


import demo_chat
from enum import Enum

DEFAULT_SYSTEM_PROMPT = '''
You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.
'''.strip()

# Set the title of the demo
st.title("ChatGLM3 Demo")

# Add your custom text here, with smaller font size
st.markdown(
    "<sub>æ™ºè°±AI å…¬å¼€åœ¨çº¿æŠ€æœ¯æ–‡æ¡£: https://lslfd0slxc.feishu.cn/wiki/WvQbwIJ9tiPAxGk8ywDck6yfnof </sub> \n\n <sub> æ›´å¤š ChatGLM3-6B çš„ä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒæ–‡æ¡£ã€‚</sub>",
    unsafe_allow_html=True)

# ---------------------------------
from langchain.chains import RetrievalQA
from langchain.prompts.prompt import PromptTemplate
from service.chatglm_service import ChatGLMService
from knowledge_service import KnowledgeService

self.llm_service = ChatGLMService()
self.knowledge_service = KnowledgeService()
#         # è·å–å¤§è¯­è¨€æ¨¡å‹è¿”å›çš„ç­”æ¡ˆï¼ˆåŸºäºæœ¬åœ°çŸ¥è¯†åº“æŸ¥è¯¢ï¼‰
#         def get_knowledeg_based_answer(self, query,
#                                        history_len=5,
#                                        temperature=0.1,
#                                        top_p=0.9,
#                                        top_k=4,
#                                        chat_history=[]):
#             # å®šä¹‰æŸ¥è¯¢çš„æç¤ºæ¨¡æ¿æ ¼å¼ï¼š
#             prompt_template = '''
#     åŸºäºä»¥ä¸‹å·²çŸ¥ä¿¡æ¯ï¼Œç®€æ´å’Œä¸“ä¸šçš„æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
#     å¦‚æœæ— æ³•ä»ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·è¯´ "æ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜" æˆ– "æ²¡æœ‰æä¾›è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯"ï¼Œä¸å…è®¸åœ¨ç­”æ¡ˆä¸­æ·»åŠ ç¼–é€ æˆåˆ†ï¼Œç­”æ¡ˆè¯·ä½¿ç”¨ä¸­æ–‡ã€‚
#     å·²çŸ¥å†…å®¹:
#     {context}
#     é—®é¢˜:
#     {question}
#         '''
#             prompt = PromptTemplate(template=prompt_template,
#                                     input_variables=["context", "question"])
#             self.llm_service.history = chat_history[-history_len:] if history_len > 0 else []
#             self.llm_service.temperature = temperature
#             self.llm_service.top_p = top_p
#
#             # åˆ©ç”¨é¢„å…ˆå­˜åœ¨çš„è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å™¨æ¥åˆ›å»ºå¹¶åˆå§‹åŒ–BaseRetrievalQAç±»çš„å®ä¾‹
#             knowledge_chain = RetrievalQA.from_llm(
#                 llm=self.llm_service,
#                 # åŸºäºæœ¬åœ°çŸ¥è¯†åº“æ„å»ºä¸€ä¸ªæ£€ç´¢å™¨ï¼Œå¹¶ä»…è¿”å›top_kçš„ç»“æœ
#                 retriever=self.knowledge_service.knowledge_base.as_retriever(
#                     search_kwargs={"k": top_k}),
#                 prompt=prompt)
#             # combine_documents_chainçš„ä½œç”¨æ˜¯å°†æŸ¥è¯¢è¿”å›çš„æ–‡æ¡£å†…å®¹ï¼ˆpage_contentï¼‰åˆå¹¶åˆ°ä¸€èµ·ä½œä¸ºpromptä¸­contextçš„å€¼
#             # å°†combine_documents_chainçš„åˆå¹¶æ–‡æ¡£å†…å®¹æ”¹ä¸º{page_content}
#
#             knowledge_chain.combine_documents_chain.document_prompt = PromptTemplate(
#                 input_variables=["page_content"], template="{page_content}")
#
#             # è¿”å›ç»“æœä¸­æ˜¯å¦åŒ…å«æºæ–‡æ¡£
#             knowledge_chain.return_source_documents = True
#
#             # ä¼ å…¥é—®é¢˜å†…å®¹è¿›è¡ŒæŸ¥è¯¢
#             result = knowledge_chain({"query": query})
#             return result
#
#         # è·å–å¤§è¯­è¨€æ¨¡å‹è¿”å›çš„ç­”æ¡ˆï¼ˆæœªåŸºäºæœ¬åœ°çŸ¥è¯†åº“æŸ¥è¯¢ï¼‰
#         def get_llm_answer(self, query):
#             result = self.llm_service._call(query)
#             return result

# ---------------------------------
class Mode(str, Enum):
    CHAT = 'ğŸ’¬ Chat'


with st.sidebar:
    top_p = st.slider(
        'top_p', 0.0, 1.0, 0.8, step=0.01
    )
    temperature = st.slider(
        'temperature', 0.0, 1.5, 0.95, step=0.01
    )
    repetition_penalty = st.slider(
        'repetition_penalty', 0.0, 2.0, 1.1, step=0.01
    )
    max_new_token = st.slider(
        'Output length', 5, 32000, 256, step=1
    )

    cols = st.columns(2)
    export_btn = cols[0]
    clear_history = cols[1].button("Clear History", use_container_width=True)
    retry = export_btn.button("Retry", use_container_width=True)

    system_prompt = st.text_area(
        label="System Prompt (Only for chat mode)",
        height=300,
        value=DEFAULT_SYSTEM_PROMPT,
    )

prompt_text = st.chat_input(
    'Chat with ChatGLM3!',
    key='chat_input',
)

tab = st.radio(
    'Mode',
    [mode.value for mode in Mode],
    horizontal=True,
    label_visibility='hidden',
)

if clear_history or retry:
    prompt_text = ""

match tab:
    case Mode.CHAT:
        demo_chat.main(
            retry=retry,
            top_p=top_p,
            temperature=temperature,
            prompt_text=prompt_text,
            system_prompt=system_prompt,
            repetition_penalty=repetition_penalty,
            max_new_tokens=max_new_token
        )

    case _:
        st.error(f'Unexpected tab: {tab}')
